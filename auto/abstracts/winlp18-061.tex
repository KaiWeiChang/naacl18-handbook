We report results of a comparison of the accuracy of crowdworkers and four Natural Language Processing (NLP) toolkits in solving two important NLP tasks, named-entity recognition and entity-level sentiment analysis. We here focus on a challenging dataset, 1,000 political tweets that were collected during the U.S. presidential primary election in February 2016. Each tweet refers to at least one of four presidential candidates, i.e., four named entities. The ground truth, established by experts in political communication, has entity-level sentiment information for each candidate mentioned in the tweet. We tested one commercial NLP toolkit, Google Cloud NL API, and three academic systems, tensiStrength, TwitterNLP, and Stanford NLP NER. Our experiments show that the commercial NLP tool was more accurate than the academic systems but did not match the accuracy of crowdworkers by a large margin for entity-level sentiment analysis. It performed almost on par with crowdworkers with regards to named-entity recognition in political tweets.
