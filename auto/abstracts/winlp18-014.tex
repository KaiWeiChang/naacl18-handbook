Information extraction today faces new challenges with noisy, short, unstructured data. These challenges are created in huge proportions on a daily basis. This is especially the case for social media messages, such as tweets, in which language can be erroneous or cryptic, and contains references to a great number of new entities. Traditional NER systems are challenged and need to develop new strategies to handle with these data. To this end, we experimented the use of both unsupervised word clusters (w2v) and online lexicon gathering  (\textit{mediawiki}), to provide lexical groundings to a classical CRF system. This system has been tested on both French (CAp'2017) and English (WNUT'2016) NER competition datasets. It obtained very good results with a minimal training and development cost. The paper describes the method, the system design, the evaluation campaign dataset and the obtained results.
