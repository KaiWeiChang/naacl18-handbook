Learning vocabulary is essential to successful communication. Complicating the task is the underappreciated fact that most common words are associated with multiple senses. While a relatively low proportion of words have senses that are unrelated to one another (e.g., flying bat vs. baseball bat), very many words evoke distinct but related senses (are polysemous) (e.g., baseball cap vs. cap of a bottle). Models of human word learning have thus far failed to represent this level of naturalistic complexity. We propose an an extension to a feature-based computational model that allows word senses to be represented distinctly (by a set of structured features), while also allowing overlap among senses to be captured (by comparisons of sets of sets). The model is shown to closely match human performance on a novel word learning task, in which people learn polysemous senses more easily than ambiguous meanings.
