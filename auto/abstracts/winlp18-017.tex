This work investigates a zero-shot translation (directions not observed at training time) in a multilingual setting. We propose an iterative training procedure that leverages a duality of translations directly generated by the system (i.e source→target and target→source), to be used together with the original parallel data to feed and iteratively re-train the multilingual network. Over time, this allows the system to learn from its own generated and increasingly better output. Our approach shows to be effective in improving the zero-shot directions of the multilingual model. In particular, we observed gains of about 9 BLEU points over a baseline multilingual model and up to 2.08 BLEU over a pivoting mechanism using two bilingual models.
