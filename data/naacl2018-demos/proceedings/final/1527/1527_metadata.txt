SubmissionNumber#=%=#1527
FinalPaperTitle#=%=#Vis-Eval Metric Viewer: A Visualisation Tool for Inspecting and Evaluating Metric Scores of Machine Translation Output
ShortPaperTitle#=%=#Vis-Eval Metric Viewer: A Visualisation Tool for Inspecting and Evaluating Metric Scores
NumberOfPages#=%=#5
CopyrightSigned#=%=#David Steele
JobTitle#==#
Organization#==#Department of Computer Science
The University of Sheffield
Regent Court
211 Portobello
Sheffield
S1 4DP
UK
Abstract#==#Machine Translation systems are usually evaluated and compared using automated evaluation metrics such as BLEU and METEOR to score the generated translations against human translations. However, the interaction with the output from the metrics is relatively limited and results are commonly a single score along with a few additional statistics. Whilst this may be enough for system comparison it does not provide much  useful feedback or a means for inspecting translations and their respective scores. 

VisEval Metric Viewer {VEMV} is a tool designed to provide visualisation of multiple evaluation scores so they can be easily interpreted by a user. VEMV takes in the source, reference, and hypothesis files as parameters, and scores the hypotheses using several popular evaluation metrics simultaneously. Scores are produced at both the sentence and dataset level and results are written locally to a series of HTML files that can be viewed on a web browser. The individual scored sentences can easily be inspected using powerful search and selection functions and results can be visualised with graphical representations of the scores and distributions.
Author{1}{Firstname}#=%=#David
Author{1}{Lastname}#=%=#Steele
Author{1}{Email}#=%=#dbsteele1@sheffield.ac.uk
Author{1}{Affiliation}#=%=#The University of Sheffield
Author{2}{Firstname}#=%=#Lucia
Author{2}{Lastname}#=%=#Specia
Author{2}{Email}#=%=#l.specia@sheffield.ac.uk
Author{2}{Affiliation}#=%=#University of Sheffield

==========