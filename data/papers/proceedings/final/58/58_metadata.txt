SubmissionNumber#=%=#58
FinalPaperTitle#=%=#Pivot Based Language Modeling for Improved Neural Domain Adaptation
ShortPaperTitle#=%=#Pivot Based Language Modeling for Improved Neural Domain Adaptation
NumberOfPages#=%=#11
CopyrightSigned#=%=#Roi Reichart
JobTitle#==#Assistant Professor
Organization#==#Technion, Israel Institute of Technology, Haifa, 3200003, Israel
Abstract#==#Representation learning with pivot-based
methods and with Neural Networks (NNs)
have lead to significant progress in domain
adaptation for Natural Language Processing.
However, most previous work that follows
these approaches does not explicitly exploit
the structure of the input text, and its output
is most often a single representation vector
for the entire text. In this paper we present
the Pivot Based Language Model (PBLM),
a representation learning model that marries
together pivot-based and NN modeling in
a structure aware manner. Particularly, our
model processes the information in the text
with a sequential NN (LSTM) and its output
consists of a representation vector for every
input word. Unlike most previous representation
learning models in domain adaptation,
PBLM can naturally feed structure aware
text classifiers such as LSTM and CNN. We
experiment with the task of cross-domain
sentiment classification on 20 domain pairs
and show substantial improvements over
strong baselines.
Author{1}{Firstname}#=%=#Yftah
Author{1}{Lastname}#=%=#Ziser
Author{1}{Email}#=%=#yftah89@gmail.com
Author{1}{Affiliation}#=%=#Technion
Author{2}{Firstname}#=%=#Roi
Author{2}{Lastname}#=%=#Reichart
Author{2}{Email}#=%=#roireichart@gmail.com
Author{2}{Affiliation}#=%=#Technion

==========