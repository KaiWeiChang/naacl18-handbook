SubmissionNumber#=%=#312
FinalPaperTitle#=%=#Target Foresight Based Attention for Neural Machine Translation
ShortPaperTitle#=%=#Target Foresight Based Attention for Neural Machine Translation
NumberOfPages#=%=#11
CopyrightSigned#=%=#Xintong Li
JobTitle#==#
Organization#==#The Chinese University of Hong Kong
The Chinese University of Hong Kong, Shatin N.T., Hong Kong
Abstract#==#In neural machine translation, an attention model is used to identify the
aligned source words for a target word （target foresight word） in order to
select translation context, but it does not make use of any information of this
target foresight word at all. Previous work proposed an approach to improve the
attention model by explicitly accessing this target foresight word and
demonstrated the substantial gains in alignment task. However, this approach is
useless in machine translation task on which the target foresight word is
unavailable. In this paper, we propose a new attention model enhanced by the
implicit information of target foresight word oriented to both alignment and
translation tasks. Empirical experiments on Chinese-to-English and
Japanese-to-English datasets show that the proposed attention model delivers
significant improvements in terms of both alignment error rate and BLEU.
Author{1}{Firstname}#=%=#Xintong
Author{1}{Lastname}#=%=#Li
Author{1}{Email}#=%=#xtli@ee.cuhk.edu.hk
Author{1}{Affiliation}#=%=#The Chinese University of Hong Kong
Author{2}{Firstname}#=%=#Lemao
Author{2}{Lastname}#=%=#Liu
Author{2}{Email}#=%=#lemaoliu@gmail.com
Author{2}{Affiliation}#=%=#Tencent AI Lab
Author{3}{Firstname}#=%=#Zhaopeng
Author{3}{Lastname}#=%=#Tu
Author{3}{Email}#=%=#tuzhaopeng@gmail.com
Author{3}{Affiliation}#=%=#Tencent AI Lab
Author{4}{Firstname}#=%=#Shuming
Author{4}{Lastname}#=%=#Shi
Author{4}{Email}#=%=#shumingshi@tencent.com
Author{4}{Affiliation}#=%=#Tencent AI Lab
Author{5}{Firstname}#=%=#Max
Author{5}{Lastname}#=%=#Meng
Author{5}{Email}#=%=#max.meng@ieee.org
Author{5}{Affiliation}#=%=#The Chinese University of Hong Kong

==========