SubmissionNumber#=%=#461
FinalPaperTitle#=%=#Quantifying the Visual Concreteness of Words and Topics in Multimodal Datasets
ShortPaperTitle#=%=#Quantifying the Visual Concreteness of Words and Topics in Multimodal Datasets
NumberOfPages#=%=#12
CopyrightSigned#=%=#Jack Hessel
JobTitle#==#
Organization#==#Cornell University
Ithaca, NY 14853
Abstract#==#Multimodal machine learning algorithms aim to learn visual-textual correspondences. Previous work suggests that concepts with concrete visual manifestations may be easier to learn than concepts with abstract ones. We give an algorithm for automatically computing the visual concreteness of words and topics within multimodal datasets. We apply the approach in four settings, ranging from image captions to images/text scraped from historical books. In addition to enabling explorations of concepts in multimodal datasets, our concreteness scores predict the capacity of machine learning algorithms to learn textual/visual relationships. We find that 1) concrete concepts are indeed easier to learn; 2) the large number of algorithms we consider have similar failure cases; 3) the precise positive relationship between concreteness and performance varies between datasets. We conclude with recommendations for using concreteness scores to facilitate future multimodal research.
Author{1}{Firstname}#=%=#Jack
Author{1}{Lastname}#=%=#Hessel
Author{1}{Email}#=%=#jhessel@cs.cornell.edu
Author{1}{Affiliation}#=%=#cornell.edu
Author{2}{Firstname}#=%=#David
Author{2}{Lastname}#=%=#Mimno
Author{2}{Email}#=%=#david.mimno@gmail.com
Author{2}{Affiliation}#=%=#Cornell University
Author{3}{Firstname}#=%=#Lillian
Author{3}{Lastname}#=%=#Lee
Author{3}{Email}#=%=#llee@cs.cornell.edu
Author{3}{Affiliation}#=%=#Cornell University

==========