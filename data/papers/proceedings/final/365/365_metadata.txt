SubmissionNumber#=%=#365
FinalPaperTitle#=%=#Bootstrapping Generators from Noisy Data
ShortPaperTitle#=%=#Bootstrapping Generators from Noisy Data
NumberOfPages#=%=#12
CopyrightSigned#=%=#Laura Perez
JobTitle#==#
Organization#==#University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
Abstract#==#A core step in statistical data-to-text generation concerns learning correspondences between structured data representations (e.g., facts in a database) and associated texts. In this paper we aim to bootstrap generators from large scale datasets where the data (e.g., DBPedia facts) and related texts (e.g., Wikipedia abstracts) are loosely aligned. We tackle this challenging task by introducing a special-purpose content selection mechanism. We use multi-instance learning to automatically discover correspondences between data and text pairs and show how these can be used to enhance the content signal while training an encoder-decoder architecture. Experimental results demonstrate that models trained with content-specific objectives improve upon a vanilla encoder-decoder which solely relies on soft attention.
Author{1}{Firstname}#=%=#Laura
Author{1}{Lastname}#=%=#Perez-Beltrachini
Author{1}{Email}#=%=#lperez@inf.ed.ac.uk
Author{1}{Affiliation}#=%=#School of Informatics, University of Edinburgh
Author{2}{Firstname}#=%=#Mirella
Author{2}{Lastname}#=%=#Lapata
Author{2}{Email}#=%=#mlap@inf.ed.ac.uk
Author{2}{Affiliation}#=%=#University of Edinburgh

==========