SubmissionNumber#=%=#561
FinalPaperTitle#=%=#Learning Word Embeddings for Low-resource Languages by PU Learning
ShortPaperTitle#=%=#Learning Word Embeddings for Low-resource Languages by PU Learning
NumberOfPages#=%=#11
CopyrightSigned#=%=#Chao Jiang
JobTitle#==#
Organization#==#Chao Jiang University of Virginia
Abstract#==#Word embedding is a key component
in many downstream applications in processing
natural languages. Existing approaches
often assume the existence of a large collection
of text for learning effective word embedding.
However, such a corpus may not be
available for some low-resource languages. In
this paper, we study how to effectively learn
a word embedding model on a corpus with
only a few million tokens. In such a situation,
the co-occurrence matrix is sparse as the
co-occurrences of many word pairs are unobserved.
In contrast to existing approaches often
only sample a few unobserved word pairs
as negative samples, we argue that the zero
entries in the co-occurrence matrix also provide
valuable information. We then design
a Positive-Unlabeled Learning (PU-Learning)
approach to factorize the co-occurrence matrix
and validate the proposed approaches in four
different languages.
Author{1}{Firstname}#=%=#Chao
Author{1}{Lastname}#=%=#Jiang
Author{1}{Email}#=%=#cj7an@virginia.edu
Author{1}{Affiliation}#=%=#University of Virginia
Author{2}{Firstname}#=%=#Hsiang-Fu
Author{2}{Lastname}#=%=#Yu
Author{2}{Email}#=%=#rofuyu@cs.utexas.edu
Author{2}{Affiliation}#=%=#Amazon
Author{3}{Firstname}#=%=#Cho-Jui
Author{3}{Lastname}#=%=#Hsieh
Author{3}{Email}#=%=#chohsieh@ucdavis.edu
Author{3}{Affiliation}#=%=#University of California, Davis
Author{4}{Firstname}#=%=#Kai-Wei
Author{4}{Lastname}#=%=#Chang
Author{4}{Email}#=%=#kw@kwchang.net
Author{4}{Affiliation}#=%=#UCLA

==========