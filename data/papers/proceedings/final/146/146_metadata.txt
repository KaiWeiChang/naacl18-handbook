SubmissionNumber#=%=#146
FinalPaperTitle#=%=#Comparing Automatic and Human Evaluation of Local Explanations for Text Classification
ShortPaperTitle#=%=#Comparing Automatic and Human Evaluation of Local Explanations for Text Classification
NumberOfPages#=%=#10
CopyrightSigned#=%=#Dong Nguyen
JobTitle#==#
Organization#==#The Alan Turing Institute

British Library
96 Euston Road

NW1 2DB London
Abstract#==#Text classification models are  becoming increasingly complex and opaque, however for many applications it is essential that the models are interpretable. Recently, a variety of approaches have been proposed for generating local explanations. While robust evaluations are needed to drive further progress, so far it is unclear which evaluation approaches are suitable.  This paper is a first step towards more robust evaluations of local explanations. We evaluate a variety of local explanation approaches using automatic measures based on word deletion. Furthermore, we show that an evaluation using a crowdsourcing experiment correlates moderately with these automatic measures and that a variety of other factors also impact the human judgements.
Author{1}{Firstname}#=%=#Dong
Author{1}{Lastname}#=%=#Nguyen
Author{1}{Email}#=%=#dong.p.ng@gmail.com
Author{1}{Affiliation}#=%=#Alan Turing Institute

==========