%
% File naaclhlt2018.tex
%
%% Based on the style files for NAACL-HLT 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
%\usepackage[nohyperref]{naaclhlt2018}
\usepackage{naaclhlt2018}

\input{./packages.tex}
\input{./macro.tex}

\usepackage{times}
\usepackage{latexsym}
\usepackage{url}

\mathtoolsset{showonlyrefs=true}



\aclfinalcopy % Uncomment this line for the final submission
\def\aclpaperid{226} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Provable Fast Greedy Compressive Summarization\\with Any Monotone Submodular Function}

\author{
	Shinsaku Sakaue \quad Tsutomu Hirao \quad Masaaki Nishino \quad Masaaki Nagata\\
	 NTT Communication Science Laboratories \\
	 {\tt \{sakaue.shinsaku, hirao.tsutomu\}@lab.ntt.co.jp} \\
	 {\tt \{nishino.masaaki, nagata.masaaki\}@lab.ntt.co.jp}
%Shinsaku Sakaue \\
%	Affiliation / Address line 1 \\
%	Affiliation / Address line 2 \\
%	Affiliation / Address line 3 \\
%	{\tt email@domain} 
%	\\\And
%	Tsutomu Hirao \\
%	Affiliation / Address line 1 \\
%	Affiliation / Address line 2 \\
%	Affiliation / Address line 3 \\
%	{\tt email@domain} 
%	\\\And
%	Masaaki Nishino \\
%	Affiliation / Address line 1 \\
%	Affiliation / Address line 2 \\
%	Affiliation / Address line 3 \\
%	{\tt email@domain}
%	\\\And
%	Masaaki Nagata \\
%	Affiliation / Address line 1 \\
%	Affiliation / Address line 2 \\
%	Affiliation / Address line 3 \\
%	{\tt email@domain}
	\\}

\date{}

\begin{document}
	\maketitle
	\begin{abstract} 
		{\it Submodular maximization} with the greedy algorithm has been studied as an effective approach to {\it extractive summarization}. This approach is known to have three advantages: its applicability to many useful submodular objective functions, the efficiency of the greedy algorithm, and the provable performance guarantee. However, when it comes to {\it compressive summarization}, we are currently missing a counterpart of the extractive method based on submodularity. In this paper, we propose a fast greedy method for compressive summarization. Our method is applicable to any {monotone submodular} objective function, including many functions well-suited for document summarization. We provide an approximation guarantee of our greedy algorithm. Experiments show that our method is about 100 to 400 times faster than an existing method based on integer-linear-programming (ILP) formulations and that our method empirically achieves more than $95$\%-approximation. 
	\end{abstract}
	%Compressive summarization has been extensively studied as an effective approach to obtaining concise and informative summaries. Many existing methods formulated this task as optimization problems and obtained summaries by solving them. Unfortunately, however, the class of objective functions to which those methods are applicable is limited; this is problematic in that it is hard to improve their performance by elaborating objective functions.
	
	%%%% specific definitions %%%%
	\newcommand{\Td}{{\bf T}} % document tree 
	\newcommand{\rd}{{\bf r}} % root of document
	\newcommand{\Ts}{{T}} % sentence tree
	\newcommand{\rs}{r} %root of sentence 
	\newcommand{\stkp}{\text{STKP}}
	\newcommand{\scskp}{\text{SCSKP}}
	
	\newcommand{\nl}{\lambda} % number of leaves
	\newcommand{\ns}{N} % number of sentences
	
	\renewcommand{\d}{d} % violated iteration
	\renewcommand{\L}{L} % budget
	\renewcommand{\l}{\ell} % cost of a chunk
	
	\newcommand{\qsub}[1]{q_{{#1}}}
	\newcommand{\qhsub}[1]{\qh_{{#1}}}
	\newcommand{\zbz}{Z^* - Z}
	
	\newcommand{\rouge}{\text{R{\small OUGE}}}
	\newcommand{\rougei}[1]{\text{$\rouge_{#1}$}}
	
	\newcommand{\J}{M}
	\newcommand{\cov}{\text{COV}}
	\newcommand{\counts}{\text{N}}
	\newcommand{\covr}{\text{COVR}}
	\newcommand{\countn}{\text{C}}
	
	\newcommand{\ours}{Greedy}
	\newcommand{\ilp}{ILP}
	
	\newcommand{\rset}{\rs_{1:N}}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%\renewcommand{\memo}[1]{{#1}}
	%\renewcommand{\rr}[1]{{#1}}
	
	
	\section{Introduction}
	%%%% automatic text summarization %%%%
	Automatic document summarization continues to be 
	a seminal subject of study 
	in natural language processing and 
	information retrieval~\cite{luhn1958automatic,edmundson1969new,cheng-lapata:2016:P16-1,peyrard-ecklekohler:2017:Long}. 
	%%%% demand for fast summarization %%%%
	Owing to the recent advances in data collection, 
	the size of document data to be summarized has been exploding, 
	which has been bringing a drastic increase in the demand for  
	fast summarization systems.  
	
	%%%% subset selection %%%%
	%	To design fast summarization systems, 
	%	{\it extractive} approaches are often employed;   
	%	namely, 
	%\rrr{textual units (e.g., words or chunks)}{sentences}
	{\it Extractive summarization} is a widely used approach 
	to designing fast summarization systems.  
	With this approach, 
	we construct a summary by extracting some sentences 
	from the original document(s).   
	%thus avoiding time-consuming text generation. 
	The extractive approach is not only fast but also 
	has the potential to achieve state-of-the-art \rouge\ scores~\cite{lin2004rouge},  
	which was revealed by~\citet{hirao2017enumeration}. 
	In many existing methods, 
	sentences   
	are extracted 
	by solving various subset selection problems: 
	for example, the
	{\it knapsack problem}~\cite{mcdonald2007study}, 
	{\it maximum coverage problem}~\cite{filatova2004formal,takamura2009cover},  
	{\it budgeted median problem}~\cite{takamura2009median}, 
	and {\it submodular maximization problem}~\cite{lin2010multi}. 
	Of particular interest, the method based on submodular maximization has three advantages: 
	(1) Many objective functions used for document summarization are known to be {\it monotone} and {\it submodular}~\cite{lin2011class,jkurisinkel-EtAl:2016:N16-2}; 
	examples of such functions include 
	the {\it coverage function}, {\it diversity reward function}, and \rouge. 
	Therefore, 
	the method can deliver high performance 
	by using monotone submodular objective functions 
	that are suitable for the given tasks. 
	(2) The efficient greedy algorithm is effective for the submodular maximization problem, 
	which provides fast summarization systems. 
	%	The submodular maximization problem can be solved approximately via the efficient greedy algorithm, 
	%	thus providing a fast summarization system.  
	(3) Theoretical performance guarantees of the greedy algorithm can be proved; 
	for example, a $\frac{1}{2}(1-e^{-1})$-approximation guarantee can be obtained.  
	
	
	%
	Although the above extractive methods successfully 
	obtain summaries with high \rouge\ scores, %~\cite{lin2004rouge}, 
	they have the following shortcoming: 
	A long sentence typically has redundant parts, 
	which means a summary constructed simply by extracting some sentences 
	often includes many redundant parts.  
	As a result, 
	if the limitation placed on summary length is tight, 
	the extractive approach cannot yield an informative summary.  
	
	%%%% existing compressive models %%%%
	%
	{\it Compressive summarization} is 
	known to be effective in overcoming this problem.    
	With this approach, a summary is constructed with some compressed sentences, 
	and thus we can obtain a concise and informative summary.  
	To make compressed sentences, 
	the dependency-tree-based approach~\cite{filippova08} is often used, 
	which is advantageous in that 
	each compressed sentence preserves its original dependency relations. 
	Specifically, 
	given a set of dependency trees constructed 
	for sentences in the original documents, 
	a summary is obtained by extracting some rooted subtrees; 
	each subtree corresponds to a compressed sentence.  
	%%% existing compressive summarization methods %%%  
	Different from the extractive summarization, 
	the dependency relations in each sentence must be taken into account,  
	and hence the aforementioned extractive methods cannot be applied to compressive summarization. 
	A number of methods have been proposed for compressive summarization~\cite{berg2011jointly,almeida2013fast,morita2013subtree,kikuchi2014single,hirao2017oracle}. 
	These methods formulate summarization   
	as a type of 
	combinatorial optimization problem with a tree constraint, 
	and they obtain summaries by solving the problem. 
	Unfortunately,  
	the existing methods have two drawbacks: 
	(1) The class of objective functions to which they are applicable is limited; 
	for example, they work only with the linear function or {coverage function}.  
	As a result, the performance of these methods cannot be improved by elaborating the objective functions.   
	(2) They contain costly procedures as their building blocks: 
	integer-linear-programming (ILP) solvers, 
	dynamic programming (DP) algorithms, and so on.  
	Therefore, they are not fast enough to be applied to large-scale document data. 
	In a nutshell, 
	compressive summarization is currently missing 
	a fast method that is applicable to a wide variety of objective functions. 
	
	
	\subsection{Our Contribution}
	%%%% our contribution %%%%
	In this paper, 
	we propose a submodularity-based greedy method 
	for compressive summarization.  
	Our method is, so to speak, a compressive counterpart 
	of the greedy method for extractive summarization~\cite{lin2010multi}. 
	Similar to the extractive method, 
	our method has the three key advantages: 
	\begin{enumerate}
		\item 
		%\\ 1. 
		Our method works with 
		any {monotone submodular} objective function, 
		a wide class of useful objective functions,      
		examples of which include 
		the coverage function, \rouge, 
		and many others~\cite{lin2011class,jkurisinkel-EtAl:2016:N16-2}. 
		\item 
		%\\ 2.  
		Our method is faster than  existing 
		compressive summarization methods since it employs 
		the efficient greedy algorithm. 
		Specifically, 
		given a set, $V$, 
		of all textual units contained in the document data  
		and a summary length limitation value, $L$,   
		our method requires at most $O(\L|V|)$ objective 
		function evaluations. 
		Experiments show that our method is 
		about 100 to 400 times faster than 
		the ILP-based method implemented with {\tt CPLEX}. 	
		\item 
		%\\ 3. 
		A theoretical guarantee of our method can be proved; 
		specifically, 
		a $\frac{1}{2}(1-e^{-1/\nl})$-approximation guarantee can be obtained, 
		where $\nl$ is a parameter defined from given document data (a definition is shown later). 
		This result generalizes 
		the $\frac{1}{2}(1-e^{-1})$-approximation of the greedy 
		algorithm for submodular maximization 
		with a knapsack constraint~\cite{leskovec2007cost}. 
		In experiments, 
		our method achieved more that 95\%-approximation. 
		Furthermore, our method attained \rougei{1} scores comparable to those of the ILP-based method.  
	\end{enumerate}  
	
	
	
	%%% submodular %%%
	\subsection{Related Work}
	There are many existing methods for compressive summarization~\cite{berg2011jointly,almeida2013fast,morita2013subtree,kikuchi2014single,hirao2017oracle}, 
	and they attempt to create summaries by solving optimization problems with 
	a tree and length constraints. 
	Unfortunately, these methods accept only a few objective functions. 
	%   specific objective functions, 
	%	and thus the class of objective functions to which they are applicable is limited. 
	
	%%% ILP %%%
	A common approach is to use ILP formulations. 
	\citet{berg2011jointly} 
	formulate the problem as an ILP 
	with the coverage objective function,  
	which is solved by using an ILP solver.  
	%
	\citet{almeida2013fast} also employs an ILP formulation  
	and solves the problem via an algorithm based on 
	{\it dual decomposition}, 
	which runs faster than an ILP 
	solver.\footnote{Their method was observed to be about 25 
		times faster than {\tt GLPK}, a commonly used free ILP solver. 
		On the other hand, {\tt CPLEX}, 
		which is a commercial ILP solver used in our experiments, 
		was observed to be about 3 to 20 times faster than {\tt GLPK}, 
		and our method is  
		about 100 to 400 times faster than {\tt CPLEX}. 
		Consequently, our method is estimated to be about 
		12 to 320 times faster than their method.}
	%
	These ILP-based methods are optimal in terms of objective function values.  
	However, it is hard to apply them to large-scale document data since to solve ILPs often takes long computation time. 
	
	%
	In an attempt to uncover the potential power of dependency-tree-based compressive summarization, 	
	\citet{hirao2017oracle} 
	solved ILPs with the \rouge\ objective function with an ILP solver.  
	Their method obtains 
	summaries by directly maximizing 
	the \rouge\ score for given reference summaries 
	(i.e., any other methods cannot achieve higher \rouge\ scores than their method).   
	The resulting summaries, called {\it oracle summaries},  
	were revealed to attain substantially high rouge scores, 
	which implies that there remains much room for further research into compressive summarization.
	
	
	A greedy method with a DP %dynamic programming (DP) 
	algorithm~\cite{morita2013subtree} is 
	probably the closest one to our idea. 
	Their method iteratively chooses compressed sentences 
	in a greedy manner,  
	for which a DP algorithm is employed.  
	Thanks to the submodularity of their objective function, 
	their method enjoys a $\frac{1}{2}(1-e^{-1})$-approximation 
	guarantee. 
	However, 
	because of the costly DP procedure, 
	their method is less scalable than the standard greedy methods  
	such as the extractive method~\cite{lin2010multi} 
	and ours. 
	Moreover, it is applicable only to objective functions that 
	are designed for their problem settings; 
	for example, it cannot use \rouge\ as an objective function. 
	
	\subsection{Overview of Our Approach}
	A high-level sketch of our approach is as follows: 
	As in many existing works,  
	we formulate the compressive summarization task 
	as a combinatorial optimization problem with a tree constraint, 
	which we call the {\it submodular tree knapsack problem} (\stkp). 
	\stkp\ is generally NP-hard; 
	in fact, it includes the knapsack problem and 
	maximum coverage problem as special cases. 
	Unfortunately, as we will see later, 
	a naive greedy algorithm for \stkp\ 
	does not offer any approximation guarantee in general. 
	The main difficulty with \stkp\ is that 
	its {tree constraint} is too complex. 
	To avoid dealing with the complex constraint directly,  
	we transform \stkp\ into a special case of the 
	{\it submodular cost submodular knapsack problem} (SCSKP)~\cite{iyer2013submodular}. 
	For general \scskp,  
	no approximation guarantee has been proved.  
	Fortunately, in our case, 
	a $\frac{1}{2}(1-e^{-1/\nl})$-approximation  
	can be proved by exploiting the structure of 
	the resulting \scskp. 
	% 	Although any approximation guarantee of the greedy algorithm 
	% 	for general \scskp s has not been proved yet, 
	% 	it can be proved in our case by
	% 	exploiting the structure of the resulting \scskp. 
	Thus we obtain a fast greedy method for 
	compressive summarization, 
	which  
	works with various monotone submodular objective functions
	and enjoys an approximation guarantee. 
	
	%	A similar approach is employed in~\cite{morita2013subtree}.  
	%	In their approach, however, we need to choose 
	%	a rooted subtree greedily out of the set of all rooted subtrees, 
	%	whose size can be exponential in $|V|$, 
	%	the number of all textual units in the document data. 
	%	As a result, their method requires costly DP procedures. 
	%	In our approach, we reformulate the \stkp\ 
	%	over a set of all rooted paths, whose size is equal to $|V|$.  
	%	Therefore, our method takes almost the same 
	%	running time as the standard greedy algorithm. 
	
	
	
	\section{Submodularity}
	%%%% extractive and compressive summatization %%%%
	
	%%%% constrained submodular summarization %%%%
	
	%{\bf Submodularity}: 
	Given finite set $V$ (e.g., a set of chunks), 
	set function $g:2^V\to\R$ is said to be {\it submodular} 
	if $g(A\cup B)+g(A\cap B)\le g(A)+g(B)$ 
	holds for any $A,B\subseteq V$.  
	We define $\gdel{A}{B}\coloneqq g(A\cup B)-g(B)$.  
	The submodularity is also characterized by the following {\it diminishing return property}: 
	$\gdel{\{v\}}{A}\ge\gdel{\{v\}}{B}$ for any $A\subseteq B$ and $v\in V\backslash B$. 
	Set function $g$ is {\it monotone} if $g(A)\le g(B)$ for any $A\subseteq B$. 
	In this paper, we focus on monotone submodular functions such that $g(\emptyset)=0$. 
	The submodularity and monotonicity are a natural fit for document summarization;  
	intuitively, 
	the marginal gain, $\gdel{\{ v \}}{S}$, 
	of adding new chunk $v\in V$ to summary $S\subseteq V$ 
	is small if $S$ already has many chunks (submodularity), 
	and a summary becomes more informative as it gets more chunks (monotonicity).  
	In fact,  
	as in~\cite{lin2011class}, 
	many objective functions well-suited for document summarization 
	have submodularity and monotonicity; 
	examples of such functions include 
	the {coverage function}, {diversity reward function}, and \rouge, 
	to name a few. 
	
	
	\begin{figure*}[htb]
		\centering
		\begin{tabular}{p{0.50\textwidth}p{0.50\textwidth}}
			\centering
			\includegraphics[width=0.45\textwidth]{figures/tree.pdf}
			\caption*{(a) Document tree $\Td=(\{ \rd \}\cup V, E)$}
			\label{fig:tree}
			&
			\centering
			\includegraphics[width=0.45\textwidth]{figures/paths.pdf}
			\caption*{(b) Set of all paths rooted at $\rd$}
			\label{fig:paths}
		\end{tabular}
		\caption{
			Illustration of the problem reformulation. 
			The left figure is a document tree 
			rooted at $\rd$;  
			it consists of two sentence trees, 
			$\Ts_1$ and $\Ts_2$, 
			rooted at $\rs_1$ and $\rs_2$, respectively.  
			We have $V=\{r_1,r_2,v_1,\dots,v_6 \}$. 
			The right figure shows $\Pl$, 
			the set of all paths rooted at $\rd$. 
			Note that $|V|=|\Pl|$ holds.  
			With our method, the greedy algorithm is 
			performed over $\Pl$, 
			which requires at most $O(|V|)$  
			objective function evaluations in each iteration. 
		}
		\label{fig:reformulation}
	\end{figure*}
	
	%%%% problem statements %%%%
	\section{Problem Statements}\label{section:problem}
	We 
	formulate the summarization task as  the following subtree extraction problem called \stkp\ hereafter. 
	In what follows, 
	we let $[M]\coloneqq \{ 1,\dots,M \}$ for any positive integer $M$. 
	
	%%%% sentence trees and document tree %%%%
	We attempt to summarize document data consisting of $\ns$ sentences. 
	Each sentence forms a dependency tree, 
	which can be constructed by using existing methods
	(e.g.,~\cite{filippova08,filippova2013overcoming}). 
	For convenience, we call 
	the dependency tree of a sentence the {\it sentence tree}.   
	The $i$-th sentence ($i\in [\ns]$)
	yields sentence tree $\Ts_i=(V_i,E_i)$ rooted at $\rs_i\in V_i$, 
	where $V_i$ is a set of textual units (e.g., words or chunks) 
	contained in the $i$-th sentence, 
	and edges in $E_i$ represent their dependency relations. 
	%which represents the rhetorical structure of the sentence.  
	We define a {\it document tree} with a dummy root vertex $\rd$ as 
	$\Td\coloneqq (\{\rd\}\cup V, E)$, where $V$ and $E$ are 
	vertex and edge sets, respectively, defined as follows: 
	\begin{align*}
		V\coloneqq \bigcup_{i\in[\ns]} V_i, & &
		E\coloneqq \bigcup_{i\in[\ns]}\{E_i\cup\{(\rd,\rs_i) \} \}. 
	\end{align*}
	Namely, $V$ is the set of all textual units contained in the document data, 
	and edges in $E$ represent the dependency relations  
	as well as the relations between $\rd$ and $\rs_i$, 
	with which the multiple sentence trees form a single document tree. 
	Figure~\ref{fig:reformulation}~(a) illustrates 
	an example of a document tree. 
	
	Given document tree $\Td$, 
	a summary preserves the original dependency relations 
	if it forms a subtree rooted at $\rd$ in $\Td$. 
	Therefore, our aim is to find a rooted subtree of $\Td$ 
	that 
	includes informative textual units.  
	%%%% cost and budget %%%%
	For each $v\in V$, the length of $v$ is denoted by $\l_v\ge0$; 
	for example, $\l_v$ is the number of words or characters in chunk $v$.
	If $S\subseteq V$ is a subset of the textual units included in an obtained summary, 
	its total length must be less than or equal to the given length limitation value $\L\ge0$; 
	namely,  
	the following {knapsack constraint} must be satisfied: 
	$\sum_{v\in S}\l_v\le\L$. 
	%%%% score function %%%%
	The quality of summary $S$ is 
	evaluated by a monotone submodular function $g$. 
	%%%% stkp %%%% 
	Consequently, compressive summarization is formulated as \stkp: 
	%the following submodular tree knapsack problem (\stkp): 
	\begin{align}\label{prob:stk}
		\maximize_{S\subseteq V}\quad  & g(S) \\
		\subto\quad  & \sum_{v\in S} \l_v\le \L,  \nonumber \\
		&\text{$S\cup\{\rd\}$ forms a subtree in $\Td$}. \nonumber   
	\end{align}
	%%%% naive greedy does not work %%%%
	At first glance, 
	it may seem that the following naive greedy approach 
	works well for this problem: 
	Starting from root $\rd$, 
	we sequentially add the most beneficial child 
	to the current solution  
	until the knapsack constraint is violated. 
	Unfortunately, the approximation ratio of 
	this method can become arbitrarily bad 
	since it may miss beneficial vertices that are far from $\rd$; 
	if such missed vertices are more beneficial 
	than those added to the solution  
	by a considerable margin, 
	the resulting approximation ratio is almost equal to zero. 
	To avoid this difficulty, 
	we reformulate \stkp\ 
	in the next section.
	
	
	\section{Proposed Method}\label{section:proposal}
	%%%% tentative definitions %%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	We observed that 
	the naive greedy algorithm does not work well 
	for \stkp~\eqref{prob:stk} 
	due to the complex tree constraint. 
	%The main difficulty of the \stkp\ is that 
	%its subtree constraint is too complex. 
	We circumvent this difficulty by 
	transforming \stkp\ 
	into a special case of the submodular cost submodular knapsack problem (\scskp). %~\cite{iyer2013submodular} 
	We then provide a greedy algorithm for \scskp. 
	%that is effective for the resulting \scskp. 
	An approximation guarantee of the greedy algorithm 
	is also presented.  
	
	
	\subsection{Problem Reformulation}
	%%%% set of paths %%%%
	We show that \stkp\ can be transformed into \scskp. 
	Let $\Pl$ be a set of all paths that connect $v\in V$ to $\rd$.  
	Note that there is a one-to-one correspondence between $v\in V$ 
	and $p\in\Pl$ that connects $v$ to $\rd$, 
	and hence $|\Pl|=|V|$.  
	%%%% path and vertex subset definition %%%%
	We define $V_p\subseteq V$ 
	as the set of vertices that are included in $p\in \Pl$, 
	and we let $V_X\coloneqq \bigcup_{p\in X} V_p$ for any $X\subseteq \Pl$. 
	%%%% equivalent transformation into scskp %%%%
	If $X\subseteq \Pl$, then $V_X\cup\{\rd\}$ forms a subtree in $\Td$. 
	Conversely, if $S\cup\{\rd\}$ forms a subtree in $\Td$ ($S\subseteq V$), 
	there exists $X\subseteq \Pl$ such that $V_X=S$. 
	Thus \stkp~\eqref{prob:stk} can be transformed into the following 
	maximization problem on $\Pl$: 
	\begin{align}\label{prob:scsk}
		\maximize_{X\subseteq \Pl}\quad  & f(X)\coloneqq g(V_X) \\
		\subto\quad  & c(X)\coloneqq \sum_{v\in V_X} \l_v\le \L. \nonumber   
	\end{align}
	We here suppose that 
	$c(p)\le \L$ holds for all $p\in \Pl$; 
	any $p\in \Pl$ violating this condition 
	can be removed in advance since 
	no feasible solution includes such $p$.  
	%
	The set functions $f$ and $c$ are 
	monotone submodular functions defined on $\Pl$ 
	(see the Appendix),  
	and thus the above problem is \scskp. 
	Figure~\ref{fig:reformulation} illustrates 
	how to transform \stkp\ into \scskp. 
	
	
	
	
	\subsection{Greedy Algorithm}
	
	We provide a greedy algorithm for \scskp~\eqref{prob:scsk}. 
	%%%% notation %%%%
	In what follows, 
	given any $X,Y\subseteq \Pl$,  
	we define the binary operators $+$ and $-$ on $\Pl$ as 
	\begin{align*}
		X+Y&\coloneqq \{p\in\Pl: p\in X \text{ and/or } p\in Y \}, \\ 
		X-Y&\coloneqq \{p\in\Pl: p\in X \text{ and } p\notin Y \}. 
	\end{align*}
	Namely, they are the union and subtraction of
	two subsets defined on $\Pl$.  
	We sometimes abuse the notation and regard $p\in\Pl$ as a subset of $\Pl$; 
	for example, we let $X+p=X+\{p\}$ for any $X\subseteq \Pl$ and $p\in\Pl$. 
	Furthermore, we define $\fdel{X}{Y}\coloneqq f(X+Y)-f(Y)$ and $\cdel{X}{Y}\coloneqq c(X+Y)-c(Y)$ for any $X,Y\subseteq \Pl$. 
	
	\begin{algorithm}[tb]
		\caption{Greedy}  
		\label{alg:greedy}
		\begin{algorithmic}[1]
			\State $U\gets \Pl$, $X\gets\emptyset$ 
			\While{$U\neq\emptyset$}
			\State $p=\argmax_{p^\prime\in U}\frac{\fdel{p^\prime}{X}}{\cdel{p^\prime}{X}}$ 
			\If{$c(X+ p) \le \L$}
			\State $X\gets X+p$  
			\EndIf
			\State $U\gets U-p$ 
			\EndWhile
			\State $\ph=\argmax_{p^\prime\in \Pl}f(p^\prime)$ \\
			\Return $Y=\argmax_{X^\prime\in\{X, \ph\} } f(X^\prime) $ 
		\end{algorithmic}
	\end{algorithm}
	%
	Algorithm~\ref{alg:greedy} presents 
	a concise description of 
	the greedy algorithm for \scskp~\eqref{prob:scsk}. 
	In practice, 
	function evaluations in the above greedy algorithm 
	can be reduced by using the technique provided 
	in~\cite{leskovec2007cost} with some modifications.  
	The resulting greedy algorithm requires 
	at most $O(\L|V|)$ function evaluations. 
	
	Different from the naive greedy algorithm 
	explained in Section~\ref{section:problem}, 
	the above greedy algorithm is performed on the set of all rooted paths, $\Pl$. 
	Thus, even if beneficial vertices are far from $\rd$, 
	rooted paths that include such beneficial vertices are considered as candidates to be chosen  
	in each iteration. 
	As a result, we get the following performance guarantee for Algorithm~\ref{alg:greedy}; 
	we define $\nl_i$ as the number of leaves in $\Ts_i$ for $i\in[\ns]$,  
	and we let $\nl\coloneqq \max_{i\in[\ns]}\nl_i$.  
	%
	\begin{thm}		
		If $Y\subseteq \Pl$ is the output of Algorithm~\ref{alg:greedy} 
		and $X^*\subseteq \Pl$ is an optimal solution for \scskp~\eqref{prob:scsk}, 
		then we have $f(Y)\ge\frac{1}{2}(1-e^{-1/\nl})f(X^*)$. 
	\end{thm}
	\begin{proof}
		See the Appendix. 
	\end{proof}
	
	In other words, 
	Algorithm~\ref{alg:greedy} enjoys a $\frac{1}{2}(1-e^{-1/\lambda})$-approximation guarantee. 
	Notably, if the values of $\nl_i$ ($i\in[N]$)
	%, the number of leaves in each sentence tree, 
	are bounded by a small constant for all $\ns$ sentences, 
	the performance guarantee does not deteriorate 
	no matter how many sentences are in the document data. 
	%This is often the case with document summarization since each sentence typically has at most a limited number of words or chunks. 
	This implies that our method works effectively 
	for summarizing large-scale document data that comprises many sentences.  
	
	\subsection{Relation with Existing Work}
	We first see some existing results. 
	For submodular maximization with a size constraint 
	(i.e., $|S|$ must be at most a certain value), 
	the greedy algorithm has been proved to achieve $(1-e^{-1})$-approximation~\cite{nemhauser1978analysis}. 
	\citet{khuller1999budgeted} studied the maximum coverage 
	problem with a knapsack constraint, 
	and proved that the greedy algorithm 
	achieves $(1-e^{-1/2})$-approximation. 
	They also showed that $(1-e^{-1})$-approximation 
	can be obtained by executing the greedy algorithm 
	$O(|V|^3)$ times, 
	and this result was generalized to the case with 
	a submodular objective function~\cite{sviridenko2004note}. 
	%	; unfortunately, however, 
	%	this method takes too long computation time in practice. 
	The greedy algorithm
	for submodular maximization with a knapsack constraint 
	is known to achieve  
	$\frac{1}{2}(1-e^{-1})$-approximation~\cite{leskovec2007cost}. 
	\citet{lin2010multi} stated that $(1-e^{-1/2})$-approximation 
	can be obtained with the greedy algorithm, 
	but 
	a mistake in their proof was pointed out 
	by~\citet{morita2013subtree}.\footnote{Probably, this mistake can be fixed with the techniques used in~\cite{khuller1999budgeted}.} 
	
	Unlike the above problem settings, 
	submodular maximization with a tree constraint 
	has only a few literatures. 
	\citet{krause2006near} studied submodular maximization over a graph with a knapsack and tree constraints, but their algorithm, 
	called~{\it pSPIEL}, 
	requires a complicated preprocessing step 
	and imposes some assumptions on the problem, 
	which do not hold in most summarization tasks. 
	\citet{iyer2013submodular} addressed \scskp, 
	a more general problem setting. 
	Their algorithm is, however, 
	more expensive than the greedy algorithm, 
	and 
	it only achieves a {\it bi-criterion} approximation guarantee 
	(i.e., not only the objective value but also  
	the magnitude of constraint violation is approximated); 
	if we use this algorithm for document summarization, 
	a resulting summary may violate the length limitation.  
	%its approximation ratio highly depends on the {\it curvature} of the objective submodular function. 
	% 	To the best of our knowledge, 
	% 	no approximation guarantee of the greedy algorithm 
	% 	has been proved for 
	% 	general submodular maximization with a tree constraint. 
	
	We turn to the relation between our result and the existing ones.  
	We consider 
	submodular maximization with a knapsack constraint. 
	This problem can be formulated as an \stkp\ 
	on a {\it star graph}, 
	whose vertex and edge sets are  
	$\{\rd,\rs_1,\dots,\rs_N\}$ and 
	$\{(\rd,\rs_1),\dots,(\rd,\rs_N)\}$, respectively  
	(i.e., every leaf corresponds to an element in $V=\{\rs_1,\dots,\rs_N\}$). 
	In this case, we have $\nl=1$, 
	and thus we obtain 
	a $\frac{1}{2}(1-e^{-1})$-approximation guarantee, 
	matching the result  
	of~\cite{leskovec2007cost}.\footnote{
		We also tried to obtain an approximation guarantee that corresponds to 
		the $(1-e^{-1/2})$-approximation~\cite{khuller1999budgeted,lin2010multi}, 
		but it was not straightforward to 
		apply their techniques to our case.
	} 
	
	\section{Objective Functions}\label{section:objectives}
	As presented in~\cite{lin2011class}, 
	many objective functions used for document summarization 
	are known to be monotone and submodular. 
	Below we list examples of the functions that will be used in the experiments.  	
	
	\subsubsection*{Coverage Function} 
	To use
	the coverage function is a simple but powerful 
	approach to document summarization, 
	and so it appears in many existing 
	works~(e.g.,~\cite{filatova2004formal,takamura2009cover,berg2011jointly}).  
	Let $\J$ be the number of distinct words in the document data, 
	and suppose that they are indexed with $j\in[M]$. 
	We let $w_j$ ($j\in[\J]$) be the weight value of the $j$-th word. 
	Given summary $S\subseteq V$,  
	the coverage function $\cov(S)$  
	is defined as follows: 
	\begin{align*}
		\cov(S)\coloneqq \sum_{j=1}^\J w_j z_j,
	\end{align*}
	where $z_j\in\{0,1\}$ is a binary decision variable 
	that indicates whether the $j$-th word is 
	included in $S$ or not; 
	more precisely, $z_j=1$ if and only if 
	at least one textual unit in $S$ 
	contains the $j$-th word. 
	
	
	\subsubsection*{Coverage Function with Rewords}
	A summary obtained with the above 
	coverage function often consists 
	of many overly-compressed sentences, 
	which typically leads to low readability. 
	\citet{morita2013subtree} addressed this problem 
	by adding a positive reward term to the coverage function. 
	Given summary $S$, 
	let $b_{\rs_i}\in\{0,1\}$ ($i\in[N]$) be 
	a binary decision variable that indicates whether 
	$\rs_i$,  
	the root node of sentence tree $\Ts_i$, 
	is included in $S$ or not. 
	Note that, if $S\cup\{\rd\}$ forms a rooted subtree in $\Td$, 
	we have $b_{\rs_i}=1$ if and only if 
	at least one textual unit in the $i$-th sentence appears in $S$. 
	With these additional variables, 
	the modified coverage function %~\cite{morita2013subtree} 
	can be written as 
	\[
	\covr(S)\coloneqq\cov(S)+\gamma\left(\sum_{v\in S}\l_v - \sum_{i=1}^N b_{\rs_i}\right),
	\]
	% 	Let $\counts(S)$ be the number of sentences 
	% 	included in a given summary $S\subseteq V$. 
	% 	We consider the following variant of 
	% 	the coverage function: %used in~\cite{morita2013subtree}: 
	% 	\[
	% 	\covr(S)\coloneqq\cov(S)+\gamma\left(\sum_{v\in S}\l_v - \counts(S) \right),
	% 	\]
	where $\gamma\ge0$ is a parameter 
	that controls the rate of sentence compression. 
	The value of $\sum_{i=1}^N b_{\rs_i}$ is equal to  
	the number of sentences whose textual unit(s) is used in $S$.  
	Therefore, a summary that consists of 
	fewer sentences tends to get a higher objective value, 
	thus enhancing readability.   
	
	
	\subsubsection*{\rouge}
	\rouge~\cite{lin2004rouge} is widely used for summarization evaluation, and 
	it is known to be highly correlated with human evaluation. 
	Furthermore, \rouge\ is known to be monotone and submodular~\cite{lin2011class}. 
	Specifically, 
	given $K$ reference summaries $R_1,\dots,R_K\subseteq V$ and function $\countn_e(S)$, 
	which counts the number of times that $n$-gram $e$ occurs in summary $S\subseteq V$, 
	the \rougei{n} score function is defined as  
	\begin{align}
		&\rougei{n}(S) \\
		&\coloneqq \frac{\sum_{k=1}^K\sum_{e\in R_k} \min\{\countn_e(S),\countn_e(R_k)  \} }{ \sum_{k=1}^K\sum_{e\in R_k} \countn_e (R_k) }. \label{def_rougen}
	\end{align}
	
	
	%%%% table %%%%
	\begin{table*}[htb]
		%\begin{small}
		\centering
		\begin{tabular}{cc|rrr}
			Objective function                      & Method & Approximation ratio &  \rougei{1}  & Time (ms)  \\ \hline
			\multirow{2}{*}{Coverage}               & \ours   & 0.964            & {\bf 0.347}      & {\bf 1.34}       \\
			& \ilp    & {\bf 1.00}            & 0.346      & 231        \\
			\multirow{2}{*}{Coverage with rewards}  & \ours   & 0.967             & {\bf 0.334}      & {\bf 1.44}       \\
			& \ilp    & {\bf 1.00}             & 0.332      & 552 \\
			\multirow{2}{*}{\rougei{1}}      & \ours   & 0.985           & 0.468      & {\bf 0.759}      \\
			& \ilp~(oracle)   & {\bf 1.00}           & {\bf 0.494}      & 92.1        
		\end{tabular}
		\caption{Approximation ratios, \rougei{1} scores, and running times for our method (\ours) and the ILP-based method (\ilp); 
			the average values over the 50 topics are presented. The two methods are applied to compressive summarization tasks with three types of objective functions: Coverage, Coverage with rewards, and \rougei{1}. 
			%The \rougei{1} scores for the \rougei{1} objective function are the same as the corresponding objective values.  
			Summaries obtained with the ILP-based method and \rougei{1} objective function are oracle summaries.} 
		%\end{small}
		\label{tbl:info}   
	\end{table*}
	
	
	%%%%%%%%%%%%%%
	
	\section{Experiments}
	We applied our method to compressive summarization tasks with the three kinds of objective functions: 
	the coverage function, the one with rewards, and \rougei{1}. 
	To benchmark our method, 
	we also applied the ILP-based method to the tasks.    
	These two methods were compared in terms of 
	achieved approximation ratios, \rougei{1} scores, and running times. 
	
	\subsection{Settings}
	In the following experiments, 
	we regard $V$ as the set of all chunks in the document data.  
	For each chunk $v\in V$, 
	we let $\l_v$ be the number of words contained in $v$, 
	and we set the length limitation, $L$, to 100. 
	For the coverage function and the one with rewards, 
	the weight values $w_j$ ($j\in[\J]$) 
	were estimated by logistic regression~\cite{yih2007multi} 
	trained on the DUC-2003 dataset. 
	For the coverage function with rewards, we set the parameter, $\gamma$, to 0.9.  
	
	The experiments were conducted on the DUC-2004 dataset 
	for multiple document summarization evaluation, 
	which is a commonly used benchmark dataset. 
	The dataset consists of 50 topics, 
	each of which has 10 newspaper articles. 
	The dependency trees for this dataset were obtained as follows: 
	We first applied the Stanford parser~\cite{de2006generating}
	%\rr{(de2006generating)} 
	% 
	to all sentences in the dataset 
	in order to obtain dependency relations between words. 
	We then applied Filippova's rules~\cite{filippova08,filippova2013overcoming} 
	to the obtained relations 
	so as to construct trees that represent dependency relations 
	between chunks. 
	To obtain summaries with high readability, 
	we treated a set of chunks connected with certain relations 
	(e.g., subject--object) as a single chunk. 
	
	Our algorithm was implemented in C++ and 
	compiled with {\tt GCC version 4.8.5}. 
	The ILP-based method 
	solved ILPs with {\tt CPLEX ver. 12.5.1.0}, 
	a widely used commercial ILP solver. 
	The details of ILP formulations for the three objective functions are presented 
	in the Appendix. 
	%
	All experiments were conducted on a Linux machine 
	(CPU:~Intel Xeon E5-2620 v4 2.10GHz and 32GB RAM).
	
	
	\begin{table*}
		\begin{tabular}{|p{0.97\textwidth}|}
			\hline 
			{\bf Greedy:}\\
			Yeltsin suffered from disease and had a heart attack followed by multiple bypass surgery in the months.
			Russian President Boris Yeltsin cut short a trip to Central Asia on Monday due to a respiratory infection that revived questions about his health and ability to lead Russia through a sustained economic crisis.
			Doctors insisted that Yeltsin fly home ahead of schedule.
			The prime minister reiterated Wednesday that Yeltsin has plans to resign early elections.
			Russia's Constitutional Court opened hearings Thursday on whether Boris Yeltsin can seek a term.
			Sources in Primakov's office said the cancellation was due to concerns.
			\\ \hline  \hline  %\hline\\{}
			{\bf ILP:}\\
			Russian President Boris Yeltsin cut short a trip to a respiratory infection that revived questions about his health and ability to lead Russia through a economic crisis.
			Yeltsin was spending outside Moscow his spokesman Dmitry Yakushkin told reporters.
			Doctors insisted Monday that Yeltsin fly home from Central Asia ahead of schedule because he was suffering.
			Yeltsin falls ill speculation arises.
			The prime minister reiterated Wednesday that Yeltsin has plans to resign early elections.
			Russia's Constitutional Court opened hearings Thursday on whether Boris Yeltsin can seek a term.
			Sources in Primakov's office said the cancellation was due to concerns.
			\\ \hline
			%\caption*{(b) ILP}
		\end{tabular}
		\caption{Summaries obtained with our greedy method (upper) and the ILP-based method (lower) for topic:D31032. 
			To obtain these summaries, 
			both methods used the coverage function with rewards as an objective function.}
		\label{tbl:summary}
	\end{table*}
	
	
	
	\subsection{Results}
	Table~\ref{tbl:info} 
	summarizes the comparisons of the achieved approximation ratios, 
	\rougei{1} scores and running times. 
	The ILP-based method are always optimal 
	in terms of objective values (i.e., 100\%-approximation is attained), and our method achieved more than 95\%-approximation. 
	We observed that  
	the maximum number, $\nl$, of leaves in a sentence tree was about $22$ on average,   
	which leads to a $2.2$\%-approximation guarantee of our algorithm. 
	Therefore, our method empirically performs much better than the theoretical guarantee;  
	this is often the case with the greedy algorithm for submodular maximization problems, 
	in particular when the problems have complex constraints.  
	%
	The \rougei{1} scores of our method are comparable to those of the ILP-based method. 
	With the coverage function and the one with rewards, 
	it happened that our method attained slightly higher \rougei{1} scores 
	than those of ILP-based methods;\footnote{
		Similar results were observed in~\cite{takamura2009cover}.
		%		If we use other objective functions, 
		%		it is also possible that the ILP-based method outperforms our method in terms of \rougei{1} scores.
		%		To understand these results, we need further discussion on how to design objective functions, 
		%		which, however, is beyond the scope of this paper.
	} 
	note that this result is possible since the objective values and \rougei{1} scores 
	are not completely 
	correlated. The results on approximation ratios and \rougei{1} scores imply that 
	our method compares favorably with the ILP-based method in terms of empirical performance.
	%	our method attained slightly higher \rougei{1} scores 
	%	than the ILP-based method; 
	%	note that this result is possible since the objective values and \rougei{1} scores 
	%	are not completely correlated.\footnote{
	%		Similar results were observed in~\cite{takamura2009cover}. 
	%		If we use other objective functions, 
	%		it is also possible that the ILP-based method outperforms our method in terms of \rougei{1} scores.
	%		To understand these results, we need further discussion on how to design objective functions, 
	%		which, however, is beyond the scope of this paper.}  
	%	This result implies the potential effectiveness 
	%	of our approach 
	%	for compressive summarization; 
	%	in other words, 
	%	summaries obtained with our greedy method can be better, 
	%	in terms of the \rougei{1} score,  
	%	than those obtained by directly maximizing the objective functions. 
	%	%
	%	\memo{this part may be moved into related work}
	%	Summaries obtained by the ILP-based method 
	%	for the \rougei{1} objective function are called {\it oracle summaries}~\cite{hirao2017oracle}; 
	%	namely, it directly maximizes the \rougei{1} score for 
	%	given reference summaries $R_1,\dots,R_K$.  
	%	Therefore, $\rougei{1}=0.494$ is the upper-bound of 
	%	average \rougei{1} scores for this task. 
	%
	With regard to the running times, 
	our method substantially outperformed the ILP-based method. 
	Specifically, our method was about 170, 380, and 120 times faster 
	than the ILP-based one for 
	the  
	coverage function, 
	the one with rewards, 
	and the \rougei{1}\ objective function, 
	respectively. 
	
	Table~\ref{tbl:summary} shows examples of the summaries obtained by our method and the ILP-based method; 
	both methods used the coverage function with rewards as 
	an objective function. 
	We see that both methods successfully created  
	informative summaries 
	that preserve original dependency relations. 
	%\rr{
	The readability of obtained summaries is unfortunately not high enough. 
	Note that not only our method but also most compressive summarization methods suffer this problem; 
	in fact, 
	there is little difference between the two summaries obtained with our 
	method and the optimal ILP-based method with regard to readability.
	%}	
	To conclude, 
	the empirical performance of our method 
	matches that of the ILP-based method, 
	while running about 100 to 400 times faster. 
	
	\section{Conclusion and Discussion}
	We proposed a fast greedy method for compressive summarization. 
	Our method works with any monotone submodular objective function; 
	examples of such functions include 
	the coverage function, \rouge, and many others. 
	The $\frac{1}{2}(1-e^{-1/\nl})$-approximation guarantee of our method was proved, 
	which generalizes the $\frac{1}{2}(1-e^{-1})$-approximation for 
	submodular maximization with a knapsack constraint. 
	Experiments showed that 
	our greedy method empirically achieves more than 
	95\%-approximation 
	and that it runs 
	about 100 to 400 times faster than 
	the ILP-based method implemented with {\tt CPLEX}.
	With the coverage function and its variant, 
	our method attained as high 
	\rougei{1} scores as the ILP-based method. 
	
	%\rr{
	As mentioned above, 
	current 
	compressive summarization systems 
	often fail to achieve high readability, 
	and one possible approach to this problem is to develop better objective functions. 
	Since our method is applicable to various monotone submodular objective functions  
	and can find almost optimal solutions efficiently, our method would be helpful in 
	testing the performance of newly proposed objective functions. 
	Thus we believe that our method is useful for advancing the study into compressive summarization. 
	%}

	
	Interestingly, 
	\stkp\ can be seen as a variant of 
	{\it DR-submodular} maximization~\cite{soma2017non}, 
	which is a submodular maximization problem 
	defined over integer lattice. 
	The constraint that appears in DR-submodular maximization is 
	somewhat easier to deal with than that of our problem; 
	exploiting this, \citet{soma2017non} developed 
	a polynomial-time algorithm 
	that achieves roughly $\frac{1}{2}$-approximation. 
	The techniques studied in this field may be useful  
	to develop better algorithms for \stkp,  
	which we leave for future work. 
	
	
	
	
	
	\bibliographystyle{acl_natbib}
	%\bibliographystyle{named}
	\bibliography{./mybib}
	


\end{document}