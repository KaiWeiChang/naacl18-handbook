SubmissionNumber#=%=#1007
FinalPaperTitle#=%=#Self-Attention with Relative Position Representations
ShortPaperTitle#=%=#Self-Attention with Relative Position Representations
NumberOfPages#=%=#5
CopyrightSigned#=%=#Peter Shaw
JobTitle#==#
Organization#==#Google
111 8th Avenue
New York, NY 10011
Abstract#==#Relying entirely on an attention mechanism, the Transformer introduced by
Vaswani et al. (2017) achieves state-of-the-art results for machine
translation. In contrast to recurrent and convolutional neural networks, it
does not explicitly model relative or absolute position information in its
structure. Instead, it requires adding representations of absolute positions to
its inputs. In this work we present an alternative approach, extending the
self-attention mechanism to efficiently consider representations of the
relative positions, or distances between sequence elements. On the WMT 2014
English-to-German and English-to-French translation tasks, this approach yields
improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations,
respectively. Notably, we observe that combining relative and absolute position
representations yields no further improvement in translation quality. We
describe an efficient implementation of our method and cast it as an instance
of relation-aware self-attention mechanisms that can generalize to arbitrary
graph-labeled inputs.
Author{1}{Firstname}#=%=#Peter
Author{1}{Lastname}#=%=#Shaw
Author{1}{Email}#=%=#petershaw@google.com
Author{1}{Affiliation}#=%=#Google
Author{2}{Firstname}#=%=#Jakob
Author{2}{Lastname}#=%=#Uszkoreit
Author{2}{Email}#=%=#jakob@uszkoreit.net
Author{2}{Affiliation}#=%=#Google, Inc.
Author{3}{Firstname}#=%=#Ashish
Author{3}{Lastname}#=%=#Vaswani
Author{3}{Email}#=%=#vaswani@usc.edu
Author{3}{Affiliation}#=%=#University of Southern California Information Sciences Institute

==========